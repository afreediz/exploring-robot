{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyEp6H1jSCgM"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env with rewards\n",
        "env = np.array([\n",
        "    [-1, -5, -3],\n",
        "    [0, 1, 2],\n",
        "    [-1, -10, 10]\n",
        "])\n",
        "\n",
        "actions = [1, 2, 3, 4]\n",
        "n_actions = len(actions)\n",
        "index_to_action = {\n",
        "    1: 'LEFT',\n",
        "    2: 'RIGHT',\n",
        "    3: 'UP',\n",
        "    4: 'DOWN'\n",
        "}\n",
        "\n",
        "# Q-Action value array with shape (states, actions)\n",
        "QA = np.random.uniform(0, 0.1, size=(env.size, n_actions))\n",
        "\n",
        "QA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htKoJULNSPdg",
        "outputId": "2ac42943-6911-4b2c-c319-a0e789b100cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02525094, 0.04528232, 0.09610647, 0.01674091],\n",
              "       [0.04631241, 0.07643725, 0.02790487, 0.06624687],\n",
              "       [0.04098774, 0.04763261, 0.01510593, 0.07993323],\n",
              "       [0.02935304, 0.06887249, 0.04884293, 0.06766028],\n",
              "       [0.0326565 , 0.03185554, 0.02269977, 0.03870416],\n",
              "       [0.0458033 , 0.05488222, 0.07618157, 0.03863312],\n",
              "       [0.07931658, 0.07940986, 0.02716863, 0.07549088],\n",
              "       [0.04636471, 0.08668528, 0.05362158, 0.01205883],\n",
              "       [0.08547938, 0.00644371, 0.01001093, 0.09817685]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# left, right, up, down\n",
        "def get_available_actions_by_state(state_index):\n",
        "    \"\"\"Returns binary mask for available actions: [left, right, up, down]\"\"\"\n",
        "    actions_map = {\n",
        "        0: [0, 1, 0, 1],  # top-left: right, down\n",
        "        1: [1, 1, 0, 1],  # top-center: left, right, down\n",
        "        2: [1, 0, 0, 1],  # top-right: left, down\n",
        "        3: [0, 1, 1, 1],  # middle-left: right, up, down\n",
        "        4: [1, 1, 1, 1],  # middle-center: all directions\n",
        "        5: [1, 0, 1, 1],  # middle-right: left, up, down\n",
        "        6: [0, 1, 1, 0],  # bottom-left: right, up\n",
        "        7: [1, 1, 1, 0],  # bottom-center: left, right, up\n",
        "        8: [1, 0, 1, 0],  # bottom-right: left, up\n",
        "    }\n",
        "    return np.array(actions_map[state_index])\n",
        "\n",
        "def state_matrix_to_index(state):\n",
        "    return state[0] * 3 + state[1]\n",
        "\n",
        "def index_to_state_matrix(index):\n",
        "    return [index // 3, index % 3]\n",
        "\n",
        "def traverse_state_matrix(state, action):\n",
        "    \"\"\"Move in the environment based on action\"\"\"\n",
        "    new_state = state.copy()\n",
        "    if action == 1:    # left\n",
        "        new_state[1] -= 1\n",
        "    elif action == 2:  # right\n",
        "        new_state[1] += 1\n",
        "    elif action == 3:  # up\n",
        "        new_state[0] -= 1\n",
        "    elif action == 4:  # down\n",
        "        new_state[0] += 1\n",
        "    return new_state\n",
        "\n",
        "def epsilon_greedy_action(state_index, epsilon=0.1):\n",
        "    \"\"\"Select action using epsilon-greedy strategy\"\"\"\n",
        "    avail_actions = get_available_actions_by_state(state_index)\n",
        "    valid_actions = [a for i, a in enumerate(actions) if avail_actions[i]]\n",
        "\n",
        "    if np.random.random() < epsilon:\n",
        "        # Explore: random action\n",
        "        return np.random.choice(valid_actions)\n",
        "    else:\n",
        "        # Exploit: best action among valid ones\n",
        "        q_values = QA[state_index, :].copy()\n",
        "        for i, a in enumerate(actions):\n",
        "            if not avail_actions[i]:\n",
        "                q_values[i] = -np.inf  # Mask invalid actions\n",
        "        best_action_idx = np.argmax(q_values)\n",
        "        return actions[best_action_idx]"
      ],
      "metadata": {
        "id": "u35ncQpeSQ-u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero out Q-values for unavailable actions\n",
        "for s in range(env.size):\n",
        "    avail_actions = get_available_actions_by_state(s)\n",
        "    QA[s, :] = QA[s, :] * avail_actions\n",
        "\n",
        "QA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Xcl3diSU5V",
        "outputId": "a7f4d52b-941e-40e7-cca9-8a7a9475c3a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.04528232, 0.        , 0.01674091],\n",
              "       [0.04631241, 0.07643725, 0.        , 0.06624687],\n",
              "       [0.04098774, 0.        , 0.        , 0.07993323],\n",
              "       [0.        , 0.06887249, 0.04884293, 0.06766028],\n",
              "       [0.0326565 , 0.03185554, 0.02269977, 0.03870416],\n",
              "       [0.0458033 , 0.        , 0.07618157, 0.03863312],\n",
              "       [0.        , 0.07940986, 0.02716863, 0.        ],\n",
              "       [0.04636471, 0.08668528, 0.05362158, 0.        ],\n",
              "       [0.08547938, 0.        , 0.01001093, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1      # Alpha - how much we update Q-values\n",
        "discount_factor = 0.9    # Gamma - importance of future rewards\n",
        "epsilon = 0.1           # Exploration rate\n",
        "n_episodes = 1000       # Number of training episodes\n",
        "max_steps = 100         # Maximum steps per episode\n",
        "\n",
        "# Track training progress\n",
        "episode_rewards = []\n",
        "episode_steps = []\n",
        "\n",
        "def epsilon_greedy_policy(state_index, epsilon):\n",
        "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
        "    available_actions = get_available_actions_by_state(state_index)\n",
        "    available_indices = np.where(available_actions == 1)[0]\n",
        "\n",
        "    if np.random.random() < epsilon:\n",
        "        # Explore: choose random available action\n",
        "        action_idx = np.random.choice(available_indices)\n",
        "    else:\n",
        "        # Exploit: choose best available action\n",
        "        q_values_masked = QA[state_index, :] * available_actions\n",
        "        # Set unavailable actions to very low value\n",
        "        q_values_masked[available_actions == 0] = -np.inf\n",
        "        action_idx = np.argmax(q_values_masked)\n",
        "\n",
        "    return action_idx + 1  # Convert to action (1-4)\n",
        "\n",
        "def get_reward(state):\n",
        "    \"\"\"Get reward for being in a state\"\"\"\n",
        "    return env[state[0], state[1]]\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting Q-learning training...\")\n",
        "for episode in range(n_episodes):\n",
        "    # Initialize episode\n",
        "    state = [0, 0]  # Start at top-left\n",
        "    state_index = state_matrix_to_index(state)\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        action = epsilon_greedy_policy(state_index, epsilon)\n",
        "\n",
        "        # Take action and observe next state\n",
        "        next_state = traverse_state_matrix(state, action)\n",
        "        next_state_index = state_matrix_to_index(next_state)\n",
        "\n",
        "        # Get reward\n",
        "        reward = get_reward(next_state)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Q-learning update\n",
        "        action_idx = action - 1  # Convert to 0-based index\n",
        "\n",
        "        # Get max Q-value for next state (considering only available actions)\n",
        "        next_available_actions = get_available_actions_by_state(next_state_index)\n",
        "        next_q_values = QA[next_state_index, :] * next_available_actions\n",
        "        next_q_values[next_available_actions == 0] = -np.inf\n",
        "        max_next_q = np.max(next_q_values) if np.any(next_available_actions) else 0\n",
        "\n",
        "        # Q-learning update rule: Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]\n",
        "        QA[state_index, action_idx] += learning_rate * (\n",
        "            reward + discount_factor * max_next_q - QA[state_index, action_idx]\n",
        "        )\n",
        "\n",
        "        # Move to next state\n",
        "        state = next_state\n",
        "        state_index = next_state_index\n",
        "        steps += 1\n",
        "\n",
        "        # Terminal condition: reached high reward state or max steps\n",
        "        if reward == 10 or steps >= max_steps:\n",
        "            break\n",
        "\n",
        "    # Track progress\n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_steps.append(steps)\n",
        "\n",
        "    # Decay exploration rate\n",
        "    epsilon = max(0.01, epsilon * 0.995)\n",
        "\n",
        "    # Print progress every 100 episodes\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-100:])\n",
        "        avg_steps = np.mean(episode_steps[-100:])\n",
        "        print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, Avg Steps = {avg_steps:.2f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i6XiaIhz14q",
        "outputId": "23974d70-89cb-48b1-ecf6-606471a609c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Q-learning training...\n",
            "Episode 100: Avg Reward = 14.43, Avg Steps = 14.18\n",
            "Episode 200: Avg Reward = 12.89, Avg Steps = 4.20\n",
            "Episode 300: Avg Reward = 12.71, Avg Steps = 4.10\n",
            "Episode 400: Avg Reward = 12.79, Avg Steps = 4.08\n",
            "Episode 500: Avg Reward = 13.04, Avg Steps = 4.08\n",
            "Episode 600: Avg Reward = 12.92, Avg Steps = 4.04\n",
            "Episode 700: Avg Reward = 12.98, Avg Steps = 4.04\n",
            "Episode 800: Avg Reward = 13.01, Avg Steps = 4.06\n",
            "Episode 900: Avg Reward = 12.98, Avg Steps = 4.04\n",
            "Episode 1000: Avg Reward = 12.87, Avg Steps = 4.04\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKrYsLvhtKDy",
        "outputId": "1637b632-72bf-473a-9b84-78595d4946dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 2.32564103e+01, 0.00000000e+00, 2.82564103e+01],\n",
              "       [2.58435897e+01, 2.57435897e+01, 0.00000000e+00, 2.97435897e+01],\n",
              "       [2.32564103e+01, 0.00000000e+00, 0.00000000e+00, 3.02564103e+01],\n",
              "       [0.00000000e+00, 2.97435897e+01, 2.58435897e+01, 2.58435897e+01],\n",
              "       [2.82564103e+01, 3.02564103e+01, 2.32564103e+01, 1.82564103e+01],\n",
              "       [2.97435897e+01, 0.00000000e+00, 2.57435897e+01, 1.00527178e+01],\n",
              "       [0.00000000e+00, 1.82564096e+01, 2.82564103e+01, 0.00000000e+00],\n",
              "       [2.58435897e+01, 1.00527178e+01, 2.97435897e+01, 0.00000000e+00],\n",
              "       [1.02101399e-03, 0.00000000e+00, 5.54924660e-02, 0.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in range(QA.shape[0]):\n",
        "  print(f\"state {r} best action {index_to_action[QA[r, :].argmax() + 1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKJpsfT1SfkW",
        "outputId": "b23562a9-1207-43dc-bc58-c16cf85e47e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state 0 best action DOWN\n",
            "state 1 best action DOWN\n",
            "state 2 best action DOWN\n",
            "state 3 best action RIGHT\n",
            "state 4 best action RIGHT\n",
            "state 5 best action DOWN\n",
            "state 6 best action UP\n",
            "state 7 best action RIGHT\n",
            "state 8 best action LEFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfKVnzWZSnU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}